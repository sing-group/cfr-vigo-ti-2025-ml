{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis utilizando `sklearn`\n",
    "\n",
    "Este notebook sirve como complemento a la clase 4 (Dimensionality Reduction) y tiene como fin ilustrar cómo se calcula la PCA para una matriz de entrada sencilla con 6 muestras y 2 variables. Utilizaremos la librería `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Comenzamos definiendo la matriz y representándola gráficamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'V1': [10, 11, 8, 3, 1, 2],\n",
    "    'V2': [6, 4, 5, 3, 2, 1]\n",
    "})\n",
    "\n",
    "print(\"Input data:\")\n",
    "display(df)\n",
    "\n",
    "df.plot.scatter(x = 'V1', y = 'V2', c = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos, la matriz de entrada debe centrarse (restar a cada valor la media de su columna/variable) antes de aplicar la PCA. Hacemos esto y representamos de nuevo el resultado gráficamente. Aunque ha cambiado la localización de los puntos, las posiciones relativas entre ellas son exactamente las mismas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.centered = df - df.mean()\n",
    "\n",
    "print(\"Centered data:\")\n",
    "display(df.centered)\n",
    "\n",
    "df.centered.plot.scatter(x = 'V1', y = 'V2', c = 'blue')\n",
    "plt.axhline(y=0, color='black', linestyle='-')\n",
    "plt.axvline(x=0, color='black', linestyle='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos la función `PCA` de sklearn para hacer el cálculo de las componentes principales y transformar los datos de entrada utilizando `fit_transform`. Podemos utilizar como entrada el DataFrame original sin centrar puesto que la librería se encarga de ello, tal y como dice en su documentación: *The input data is centered but not scaled for each feature before applying the SVD.*. En el DataFrame `points` tendremos los datos transformados al nuevo espacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "points = pca.fit_transform(df)\n",
    "points = pd.DataFrame(points)\n",
    "points.columns = ['PC1', 'PC2']\n",
    "\n",
    "print(\"Transformed data:\")\n",
    "display(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar el resto de resultados asociados a la PCA. En primer lugar, la variable `pca.components_` alberga los pesos de las componentes (loadings o weights). `components_` tiene las componentes en filas y las variables de entrada en columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"PCA Loadings:\")\n",
    "display(loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que ambas componentes son ortogonales, podemos comprobar que su producto escalar es 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dotProduct = np.dot(loadings['PC1'],loadings['PC2'])\n",
    "print(\"The dot product of the loadings is: \", dotProduct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de la PCA también contiene tres variables con la varianza explicada y el porcentaje de varianza explicada por cada componente. Imprimimos estos valores y, a mayores, el porcentaje de varianza acumulada. Recuerda que las componentes están ordenadas decrecientemente por porcentaje de varianza explicada, ya que la primera componente es la que mayor porcentaje de varianza explica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La varianza explicada por cada componente es: \", str(pca.explained_variance_))\n",
    "print(\"El porcentaje de varianza explicada por cada componente es: \", str(pca.explained_variance_ratio_))\n",
    "print(\"El acumulado del porcentaje de varianza explicada por cada componente es: \", str(pca.explained_variance_ratio_.cumsum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es instructivo comprobar cómo estas varianzas se pueden obtener a partir de la matriz de datos transformados (`points`). Calculamos las varianzas utilizando la fórmula y utilizando la función `var` del pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La varianza de PC1 es:\")\n",
    "display(sum(points['PC1'] * points['PC1']) / 5)\n",
    "\n",
    "print(\"\\nLa varianza de PC2 es:\")\n",
    "display(sum(points['PC2'] * points['PC2']) / 5)\n",
    "\n",
    "print(\"\\nLa varianza de PC1 y PC2 es:\")\n",
    "display(points.var(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para terminar con las varianzas, hacemos un `scree plot`: un gráfico de barras donde representamos el porcentaje de varianza explicad por cada PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = pd.DataFrame({'var': pca.explained_variance_ratio_, 'PC':['PC1','PC2']})\n",
    "var.plot.bar(x='PC', y=\"var\", color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acabar con el ejemplo, vamos a representar gráficamente las componentes principales y los datos transformados al nuevo espacio. Empezamos visualizando la matriz de datos de entrada (utilizamos la versión centrada) y representamos sobre ella las dos líneas que corresponden a las componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.centered.plot.scatter(x = 'V1', y = 'V2', c = 'blue')\n",
    "\n",
    "# Make sure the axis have the same graphical scale to appreciate that the two lines are orthogonal\n",
    "plt.axis('equal')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "\n",
    "# Draw the axis at 0\n",
    "plt.axhline(y=0, color='black', linestyle='-')\n",
    "plt.axvline(x=0, color='black', linestyle='-')\n",
    "\n",
    "# Draw the horizontal line for PC1. The intercept is 0 and the slope is the ratio between the PC loadings.\n",
    "\n",
    "i=0 # intercept\n",
    "s=loadings['PC1'][1]/loadings['PC1'][0] # slope\n",
    "x=np.linspace(-6,6,50)\n",
    "plt.plot(x, s*x + i)\n",
    "\n",
    "# Draw the horizontal line for PC1. The intercept is 0 and the slope is the ratio between the PC loadings.\n",
    "i=0 # intercept\n",
    "s=loadings['PC2'][1]/loadings['PC2'][0] # slope\n",
    "# s = -1/s # We know PC2 is orthogonal to PC1, so we could compute PC2's slope this way\n",
    "x=np.linspace(-2,2,50)\n",
    "plt.plot(x, s*x + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora hacemos el scatterplot de la matriz de datos transformada. Como se puede ver, la mayor variación está en el eje asociado a PC1, pues es donde los puntos se dispersan más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed data:\")\n",
    "display(points)\n",
    "\n",
    "points.plot.scatter(x = 'PC1', y = 'PC2', c = 'blue')\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "plt.axhline(y=0, color='black', linestyle='-')\n",
    "plt.axvline(x=0, color='black', linestyle='-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-cfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
