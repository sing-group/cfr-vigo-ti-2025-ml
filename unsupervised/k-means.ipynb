{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo básico de K-Means en Python utilizando `sklearn`\n",
    "\n",
    "En este notebook se presenta un ejemplo básico de K-Means en Python utilizando la librería `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Como es habitual, empezamos importando las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing \n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el DataFrame pandas con los datos de entrada: 8 muestras (filas) y 2 variables (columnas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'A': [0.93, 0.91, 0.89, 0.87, 0.19, 0.17, 0.23, 0.21], \n",
    "    'B': [0.09, 0.07, 0.13, 0.11, 0.83, 0.81, 0.79, 0.77]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representamos gráficamente la matriz para observar la naturaleza de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x = 'A', y = 'B', c = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso parece que hay dos grupos bien diferenciados, así que vamos a utilizar la función `KMeans` para agrupar las muestras en 2 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(df)\n",
    "\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las coordenadas de los dos centroides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "print('Centroide del primer cluster:', str(centroids[0,:]))\n",
    "print('Centroide del segundo cluster:', str(centroids[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y creamos nuevamente el gráfico anterior añadiendo estos dos puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x = 'A', y = 'B', c = 'black')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c = 'red', marker = 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos las asignaciones de las muestras a los clusters y las añadimos a nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(df)\n",
    "\n",
    "df['group'] = labels\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y utilizamos las etiquetas para dar un color a los puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.where(df['group']==0,'m','g')\n",
    "\n",
    "df.plot.scatter(x = 'A', y = 'B', c = colors)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c = 'red', marker = 'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar cómo la media de los puntos asignados a cada cluster se corresponde con el centroide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid0 = df[df['group'] == 0].sum(axis = 0) / 4\n",
    "centroid1 = df[df['group'] == 1].sum(axis = 0) / 4\n",
    "\n",
    "print('Centroide del primer cluster:\\n', centroid0)\n",
    "print('\\nCentroide del segundo cluster:\\n', centroid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos el método *elbow* a este ejemplo, utilizando de 1 a 3 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 4)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i) for i in k_range]\n",
    "inertia = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('inertia')\n",
    "plt.plot(k_range, inertia, linestyle='--', marker='o', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, haremos el análisis de silhouette para este caso. Para ello, sklearn incluye dos funciones: `silhouette_samples`, que devuelve los scores de todas las muestras dadas (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html), y `silhouette_score`, que devuelve el score medio sobre todas las muestras (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html). En la documentación de sklearn se puede encontrar un ejemplo de este tipo de análisis: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "Dado que esta métrica requiere de al menos dos grupos (clusters), vamos a calcular los silhouette scores solo para k = 2 y k = 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "labels_k = [kmeans[i].predict(df) for i in range(1, 3)]\n",
    "silhouettes = [metrics.silhouette_score(df, labels_k[i-1], metric = 'euclidean') for i in range(1, 3)]\n",
    "\n",
    "print('silhouette para k = 2:', silhouettes[0])\n",
    "print('silhouette para k = 3:', silhouettes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_silhouette = df.copy()\n",
    "\n",
    "df_silhouette['group (k = 2)'] = labels_k[0]\n",
    "df_silhouette['silhouette (k = 2)'] = metrics.silhouette_samples(df, labels_k[0], metric = 'euclidean')\n",
    "df_silhouette['group (k = 3)'] = labels_k[1]\n",
    "df_silhouette['silhouette (k = 3)'] = metrics.silhouette_samples(df, labels_k[1], metric = 'euclidean')\n",
    "\n",
    "df_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escalado de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como comentamos en clase de teoría, en ciertos casos puede ser necesario aplicar un escalado de variables para conseguir mejores resultados y evitar que ciertas variables dominen el método empleado. Comenzamos por añadir una nueva variable C al ejemplo anterior y aplicar el algoritmo K-Means con k = 2. Tal y como se puede ver, esta variable toma valores de 0.56 o 0.55 independientemente del \"grupo\" al que pertenece según el K-Means anterior, y el resultado de este nuevo K-Means incluyendo dicha variable es el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1 = {\n",
    "    'A': [0.93, 0.91, 0.89, 0.87, 0.19, 0.17, 0.23, 0.21], \n",
    "    'B': [0.09, 0.07, 0.13, 0.11, 0.83, 0.81, 0.79, 0.77],\n",
    "    'C': [0.55, 0.56, 0.55, 0.56, 0.55, 0.56, 0.55, 0.56]\n",
    "}\n",
    "\n",
    "df_1 = pd.DataFrame(data=d_1)\n",
    "\n",
    "kmeans_1 = KMeans(n_clusters=2).fit(df_1)\n",
    "\n",
    "df_1['group'] = kmeans_1.predict(df_1)\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, creamos un DataFrame similar pero con la variable C en una escala superior, es decir, con los valores 550 y 560 distribuidos de igual modo entre las 8 muestras. En este caso, se puede observar como estos valores tan altos dominan las distancias y se obtiene un agrupamiento diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2 = {\n",
    "    'A': [0.93, 0.91, 0.89, 0.87, 0.19, 0.17, 0.23, 0.21], \n",
    "    'B': [0.09, 0.07, 0.13, 0.11, 0.83, 0.81, 0.79, 0.77],\n",
    "    'C': [550, 560, 550, 560, 550, 560, 550, 560]\n",
    "}\n",
    "\n",
    "df_2 = pd.DataFrame(data=d_2)\n",
    "\n",
    "kmeans_2 = KMeans(n_clusters=2).fit(df_2)\n",
    "\n",
    "df_2['group'] = kmeans_2.predict(df_2)\n",
    "\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar que ciertas variables dominen el cálculo de los grupos, es habitual aplicar un escalado de variables que las sitúe en escalas comparables. La técnica más habitual es la estandarización, que consiste en escalar cada variable restando a cada valor la media y dividiéndolo por la desviación típica. De esta manera, cada variable tendrá media = 0 y desviación típica = 1. Aplicamos el escalado utilizando el `StandardScaler` de sklearn y comprobamos el resultado del agrupamiento, que vuelve a dejar de estar influenciado por la variable C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2 = {\n",
    "    'A': [0.93, 0.91, 0.89, 0.87, 0.19, 0.17, 0.23, 0.21], \n",
    "    'B': [0.09, 0.07, 0.13, 0.11, 0.83, 0.81, 0.79, 0.77],\n",
    "    'C': [550, 560, 550, 560, 550, 560, 550, 560]\n",
    "}\n",
    "df_2 = pd.DataFrame(data=d_2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_2_scaled = pd.DataFrame(scaler.fit_transform(df_2))\n",
    "\n",
    "print(df_2_scaled)\n",
    "\n",
    "print(f'Valor medio de cada columna: {scaler.mean_}')\n",
    "\n",
    "kmeans_2 = KMeans(n_clusters=2).fit(df_2_scaled)\n",
    "\n",
    "df_2_scaled['group'] = kmeans_2.predict(df_2_scaled)\n",
    "\n",
    "df_2_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-cfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
